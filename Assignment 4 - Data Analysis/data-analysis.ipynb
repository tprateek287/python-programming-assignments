{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a4 - Data Analysis\n",
    "Fill in the below code cells as specified. Note that cells may utilize variables and functions defined in previous cells; we should be able to use the `Kernal > Restart & Clear All` menu item followed by `Cell > Run All` to execute your entire notebook and see the correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Numbers\n",
    "For this part of the assignment, you will analyze some numeric data (counts of library holdings) to investiate how the distribution of numbers in natural data sets obeys the counter-intuitive [Benford's Law](https://plus.maths.org/content/os/issue9/features/benford/index). \n",
    "\n",
    "<small>(This exercise was adapted from Steve Wolfman).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable **`holdings_data`** which is a **list** of the contents of the **`data/libraryholdings.txt`** file included in the repository (each line in the file should be a single element in the list). You will need to open up the file and read its contents into a list. You should specify a _local path_ to the file (from this notebook's location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create holdings_data list as per the instructions\n",
    "holdings_data = []\n",
    "\n",
    "# Read the data from the file at path 'data/libraryholdings.txt'\n",
    "with open('data/libraryholdings.txt') as holdings_file:\n",
    "    for line in holdings_file:\n",
    "        holdings_data.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first **ten** items from the `holdings_data` list, each on its own line. (Note that there may be extra line breaks that are included in the data items themselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(* Library holdings (# of books in each library), *)\n",
      "(* collected by Christian Ayotte.                 *)\n",
      "(* Labels not available.                          *)\n",
      "\n",
      "12201\n",
      "600778\n",
      "14926\n",
      "37863\n",
      "14866\n",
      "9896\n"
     ]
    }
   ],
   "source": [
    "# Print the first ten items from the holdings_data\n",
    "for i in range(10):\n",
    "    print(holdings_data[i], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **slice operator (`:`)** to remove the \"heading\" and blank elements from the beginning of the data list, leaving only the list of numbers. The remaining values should continue to be stored (re-stored) in the `holdings_data` variable. Output the new first element in `holdings_data` to demonstrate that it is the first number in the data set.\n",
    "- The values in the list _should_ be strings rather than an integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12201\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary list to store the sliced holdings_data\n",
    "temp_holdings_data = list(holdings_data[4:])\n",
    "\n",
    "# Re-store the holdings_data variable with the temporary list created above\n",
    "holdings_data = list(temp_holdings_data)\n",
    "\n",
    "# Print the first element of holdings_data\n",
    "print(holdings_data[0], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable **`lead_digit_counts`** that is a dictionary whose keys are _strings_ of each digit (`\"0\"`, `\"1\"`, `\"2\"`, etc.), and whose values are all the number `0`. You can do this directly or with a loop. Print out the variable after you create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '6': 0, '7': 0, '8': 0, '9': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create a variable lead_digit_counts that is a dictionary\n",
    "lead_digits_counts = {}\n",
    "\n",
    "# Assign the values in the dictionary accordingly\n",
    "for i in range(10):\n",
    "    lead_digits_counts[str(i)] = 0\n",
    "    \n",
    "# Print the dictionary\n",
    "print(lead_digits_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of times each digit appears as the _first digit_ in a value of the `holdings_data` list, storing those counts in the `lead_digit_counts` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop to perform the task of extracting the first digit of each element of holdings_data\n",
    "for i in range(len(holdings_data)):\n",
    "    for k, v in lead_digits_counts.items():      # Loop through the dictionary to perform the digit comparison\n",
    "        if k == holdings_data[i][0]:\n",
    "            lead_digits_counts[k] += 1           # Increment the count upon match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a loop to print out each count in `lead_digit_counts` with the format:\n",
    "```\n",
    "X values have a leading digit of digit Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 values have a leading digit of 0\n",
      "3056 values have a leading digit of 1\n",
      "1606 values have a leading digit of 2\n",
      "1018 values have a leading digit of 3\n",
      "801 values have a leading digit of 4\n",
      "640 values have a leading digit of 5\n",
      "560 values have a leading digit of 6\n",
      "502 values have a leading digit of 7\n",
      "503 values have a leading digit of 8\n",
      "452 values have a leading digit of 9\n"
     ]
    }
   ],
   "source": [
    "# Loop to print out each count in lead_digits_counts\n",
    "for k, v in lead_digits_counts.items():\n",
    "    print(str(v) + \" values have a leading digit of \" + k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the _percentage_ of values in the the library holdings data set that have a leading digit **`1`** (round to 2 decimal places). Is this value as predicted by Benford's law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of values in the the library holdings data set that have a leading digit 1:\n",
      "33.44\n",
      "Benford's law prediction for values that have a leading digit 1:\n",
      "30.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Print percentage of values in the the library holdings data set that have a leading digit 1\n",
    "print(\"Percentage of values in the the library holdings data set that have a leading digit 1:\")\n",
    "print(round(((lead_digits_counts['1'] / len(holdings_data)) * 100), 2))\n",
    "\n",
    "# Print the expected percentage as per Benford's Law\n",
    "print(\"Benford's law prediction for values that have a leading digit 1:\")\n",
    "print(round((math.log((1 + 1) / 1, 10)) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the value as predicted by Benford's law is close to the observed percentage for the holdings_data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Extra credit challenge:*** Create a single variable `digit_position_counts` that contains the number of times that each digit 0 through 9 appears in _each_ position in the data set. E.g., a `1` appears in the 1st position 3056 times and in the second position 1005 times; a `2` appears in the 1st position 1606 times and in the second position 1044 times.\n",
    "\n",
    "Use this variable to print a \"table\" of the percentage of the time each position contains each digit (e.g., the 1st digit is a `1` 33.44% of the time, a `2` 17.57% of the time, etc).\n",
    "\n",
    "Note that for this extra challenge it is up to you to determine an appropriate data structure (e.g., how to combine dictionaries and lists and tuples) for representing this table. Be sure and include comments explaining your work.\n",
    "\n",
    "Only attempt this problem once you have completed everything else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Life Expectancy\n",
    "For this part of the assignment, you'll work with data about the life expectancy (in years) for each country in the world in the years 1960 and 2013. Note that this can be really [fun](http://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen.html) data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is found in a [.csv](https://en.wikipedia.org/wiki/Comma-separated_values) file: a plain-text data format where each line represents a record (row) of data and where feature (column) is separated by a comma.\n",
    "\n",
    "Read in the contents of the **`data/life_expectancy.csv`** data file, and use it to construct a **list** called **`life_expectancy_list`**. Each element in this list should be a **dictionary** (one for each row in the `csv` file) with the following keys and values:\n",
    "\n",
    "- a key `'country'` whose value is the name of the country (as a string)\n",
    "- a key `'le_1960'` whose value is the life expectancy in 1960 (as a float)\n",
    "- a key `'le_2013'` whose value is the life expectancy in 2013 (as a float)\n",
    "\n",
    "Thus the first record should look like:\n",
    "```\n",
    "{'country': 'Aruba', 'le_1960': 65.56936585, 'le_2013': 75.33217073}\n",
    "```\n",
    "\n",
    "You should use the **`csv`** module to read this file and break up each row into different values. See [the documentation](https://docs.python.org/3/library/csv.html) for an example of how to do this. Print out the _first row_ of your list as a demonstration that you've processed the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'csv' module\n",
    "import csv\n",
    "\n",
    "# Create the empty list - 'life_expectancy_list'\n",
    "life_expectancy_list = []\n",
    "\n",
    "# Read the 'data/life_expectancy.csv' and populate the list\n",
    "with open('data/life_expectancy.csv') as life_expectancy:\n",
    "    reader = csv.DictReader(life_expectancy)\n",
    "    for row in reader:\n",
    "        # Append the list with values extracted from the .csv file\n",
    "        life_expectancy_list.append({'country': row['country'], 'le_1960': float(row['le_1960']), \n",
    "                                     'le_2013': float(row['le_2013'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another item to each dictionary in the `life_expectancy_list` whose **key** is `change` and whose **value** is the change in life expectancy from 1960 to 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the list to create the 'change' key for each dictionary in 'life_expectancy_list'\n",
    "for i in range(len(life_expectancy_list)):\n",
    "    life_expectancy_list[i]['change'] = life_expectancy_list[i]['le_2013'] - life_expectancy_list[i]['le_1960']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable **`num_small_gain`** that stores the **number of countries** whose life expectancy did not improve by 5 years or more between 1960 and 2013. This will include counties whose life expectancy has worsened. Print out this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Create num_small_gain variable as 0\n",
    "num_small_gain = 0\n",
    "\n",
    "# Loop through the variable to figure out the countries with 'change' as less than 5\n",
    "for i in range(len(life_expectancy_list)):\n",
    "    if life_expectancy_list[i]['change'] < 5:\n",
    "        # Increment the count where True\n",
    "        num_small_gain += 1\n",
    "        \n",
    "# Print the number of countries whose life expectancy did not improve by 5 years or more between 1960 and 2013    \n",
    "print(num_small_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable **`most_improved`** that is the **name of the country** with the largest gain in life expectancy (between 1960 and 2013). Print out this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maldives\n"
     ]
    }
   ],
   "source": [
    "# Create the variable 'most_improved' with the dictionary with maximum value for 'change'\n",
    "most_improved = max(life_expectancy_list, key = lambda x: x['change'])\n",
    "\n",
    "# Print the name of the country with the largest gain in life expectancy (between 1960 and 2013)\n",
    "print(most_improved['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **`compare_country_le()`** that takes in the names of _two_ countries, and returns a **tuple** containing the following information:\n",
    "- the name of the country with the greater life expectancy,\n",
    "- the life expectancy in 2013 of that country\n",
    "- the difference between the life expectancies in 2013\n",
    "\n",
    "Use your function to print the comparison between the life expectancies of the _United States_ and _Cuba_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cuba', 79.23926829, 0.39780487999999536)\n"
     ]
    }
   ],
   "source": [
    "# Define the compare_country_le() function\n",
    "def compare_country_le(country_one, country_two):\n",
    "    \"\"\"This function accepts the names of two countries, and returns a tuple containing the following information:\n",
    "\n",
    "    - the name of the country with the greater life expectancy,\n",
    "    - the life expectancy in 2013 of that country\n",
    "    - the difference between the life expectancies in 2013\"\"\"\n",
    "    \n",
    "    country = ''\n",
    "    life_expectancy_2013_one = 0\n",
    "    life_expectancy_2013_two = 0\n",
    "    \n",
    "    # Loop through the life_expectancy_list to figure out the respective life expectancies\n",
    "    for i in range(len(life_expectancy_list)):\n",
    "        if life_expectancy_list[i]['country'] == country_one:\n",
    "            life_expectancy_2013_one = life_expectancy_list[i]['le_2013']\n",
    "        if life_expectancy_list[i]['country'] == country_two:\n",
    "            life_expectancy_2013_two = life_expectancy_list[i]['le_2013']\n",
    "            \n",
    "    # Check to figure out the country with higher life expectancy        \n",
    "    if life_expectancy_2013_one > life_expectancy_2013_two:\n",
    "        return (country_one, life_expectancy_2013_one, (life_expectancy_2013_one - life_expectancy_2013_two))\n",
    "    else:\n",
    "        return (country_two, life_expectancy_2013_two, (life_expectancy_2013_two - life_expectancy_2013_one))\n",
    "    \n",
    "# Print the outcome of the function when we pass 'United States' and 'Cuba'    \n",
    "print(compare_country_le('United States', 'Cuba'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Readability\n",
    "For this part of the assignment, you will calculate the [readability](https://en.wikipedia.org/wiki/Readability) of a text document using the [Dale-Chall Readability Formula](http://www.readabilityformulas.com/new-dale-chall-readability-formula.php). This method determines how \"easy\" it is to read a particular (English) document by considering the length of sentences and how many of the words used are \"easy\" to understand (based on a pre-defined list of \"easy\" words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting real-world text documents into words and sentences is non-trivial (English is hard!). To make this easier, you should use the [Natural Language Toolkit (nltk)](http://www.nltk.org/index.html) module. This module is included with Anacaonda, but does require some additional data source files to be installed on your computer. You _should_ be able to do this by running the below cell (you only need to run it once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('punkt')\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to load the list of \"easy\" words into memory. This list can be found in the **`data/dale-chall.txt`** file. Open this file and read its entire contents into a **list** variable (e.g., `easy_words_list`), where each element in the list is a single line (word) in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'easy_words_list'\n",
    "easy_words_list = []\n",
    "\n",
    "# Read the 'data/dale-chall.txt' to extract the easy words\n",
    "with open('data/dale-chall.txt') as dale_chall:\n",
    "    for line in dale_chall:\n",
    "        easy_words_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to \"look up\" easy words, convert the easy words list into a **dictionary** (e.g., `easy_words_dict`), where each **key** is a word, and each **value** is `True` (that the word is in the list).\n",
    "- Make sure you do not include newline characters in your keys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'easy_words_dict' dictionary where each key is a word, and each value is True\n",
    "easy_words_dict = dict(zip(easy_words_list[:], [True] * len(easy_words_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, define a dictionary **`readability_grade_dict`** to use for looking up the \"grade level\" associated with a readability score (see [this table](https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula#Formula)). This dictionary should have **keys** that are ___tuples___ giving the range of score for a particular grade (e.g., `(5.0, 5.9)`), and **values** that are ___strings___ representing the grade (e.g., `\"5th or 6th grade\"`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary 'readability_grade_dict' to use for looking up the \"grade level\" associated with a readability score\n",
    "readability_grade_dict = {\n",
    "    (0.0, 4.9):'Grade 4th or below',\n",
    "    (5.0, 5.9):'Grade 5th or 6th',\n",
    "    (6.0, 6.9):'Grade 7th or 8th',\n",
    "    (7.0, 7.9):'Grade 9th or 10th',\n",
    "    (8.0, 8.9):'Grade 11th or 12th',\n",
    "    (9.0, 9.9):'Grade 13th to 15th (college)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **`print_grade()`** that takes in a readability score (a number greater than or equal to 0), and **prints** a string representing the grade associated with that score (from your `readability_grade_dict` dictionary).\n",
    "- _Hint:_ loop through the items in the dictionary and determine which \"tuple\" key has elements that the score falls between. Be sure and round to the nearest decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the print_grade() function\n",
    "def print_grade(read_score):\n",
    "    \"\"\"This function takes in a readability score (a number greater than or equal to 0), \n",
    "    and prints a string representing the grade associated with that score\"\"\"\n",
    "    \n",
    "    for k, v in readability_grade_dict.items():\n",
    "        if round(read_score, 1) >= k[0] and round(read_score, 1) <= k[1]:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the readability scores! Define a function **`count_sentences()`** that counts the number of sentences in a string. Use the [sent_tokenize()](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize) function from the `nltk.tokenize` module to break up a string into sentences (this is like the string `split()` function, but it splits into sentences rather than dividing by spaces).\n",
    "- For help and an example, see [this guide](http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize).\n",
    "- You do not need to do any extra processing beyond that provided by the `sent_tokenize()` function.\n",
    "- Test your function on a simple pair or trio of sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'nltk' module\n",
    "import nltk\n",
    "\n",
    "# Define the 'count_sentences' function\n",
    "def count_sentences(str):\n",
    "    \"\"\"This function returns the number of sentences in a string\"\"\"\n",
    "    \n",
    "    return(len(nltk.tokenize.sent_tokenize(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **`extract_words()`** that takes in a string and returns a _list_ of all of the words in that string. Use the [word_tokenize()](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize) function from the `nltk.tokenize` module to break up the string into words.\n",
    "- The `nltk` tokenizer includes each punctuation character (e.g., commas, periods) as individual \"words\". Your list should not include these items. You can use a string method to determine whether or not the word starts with a punctuation symbol, and if so exclude it. _Hint_ think about keeping good words, rather than throwing away the bad! Note that you do not need to do any special consideration for contractions or other words that include their own punctuation.\n",
    "- Test your function on a simple sentence (with punctuation!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'string' module\n",
    "import string\n",
    "\n",
    "# Define extract_words() function\n",
    "def extract_words(str):\n",
    "    \"\"\"This function takes in a string and returns a list of all of the words in that string\"\"\"\n",
    "    \n",
    "    list_of_words = nltk.tokenize.word_tokenize(str)\n",
    "    return_list = []\n",
    "    \n",
    "    # Check if the first character is an alphabet to keep in the return list\n",
    "    for k in range(len(list_of_words)):\n",
    "        if list_of_words[k][0].isalpha():\n",
    "            return_list.append(list_of_words[k])\n",
    "    \n",
    "    # Return statement\n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **`count_easy_words()`** that takes in a _list_ of words as an argument and returns the number of words that are \"easy\".\n",
    "\n",
    "- Your function should look up each word in the `easy_words_dict` you defined earlier. _Do not look up words in the list_ (the dictionary is much faster!). Be careful to look up lowercase versions of the word.\n",
    "\n",
    "- Your function should handle detecting different parts of speech (e.g., plurals, different verb conjugations, etc.). You can do this by using the **`WordNetLemmatizer()`** function from the `nltk.stem.wordnet` module&mdash;which produces a \"lemmatizer\" object. You can call the **`lemmatize()`** method on this object to reduce a word to its \"base\" form. See [this example](https://pythonprogramming.net/lemmatizing-nltk-tutorial/). Note that you should reduce words to both their basic noun AND verb forms (you will need to call the function twice: once with `'n'` (noun) and once with `'v'` (verb) as the second argument!)\n",
    "\n",
    "- You can test your function on the word list: `['My','words','spoken','have','consequences']`, which should have 4 of the 5 words considered easy (not \"consequences\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Import the WordNetLemmatizer from nltk.stem.wordnet module\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Define the count_easy_words() function\n",
    "def count_easy_words(ls_words):\n",
    "    \"\"\"This function takes in a list of words as an argument and returns the number of words that are 'easy'\"\"\"\n",
    "    \n",
    "    return_count = 0\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Loop through each word in the list\n",
    "    for i in range(len(ls_words)):\n",
    "        \n",
    "        if lemmatizer.lemmatize(ls_words[i].lower(), 'v') in easy_words_dict.keys():\n",
    "            return_count += 1\n",
    "        elif lemmatizer.lemmatize(ls_words[i].lower(), 'n') in easy_words_dict.keys():\n",
    "            return_count += 1\n",
    "    \n",
    "    # Return statement\n",
    "    return return_count\n",
    "\n",
    "# Test the count_easy_words() function\n",
    "print(count_easy_words(['My','words','spoken','have','consequences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **`calc_readability_score()`** that takes in a string of text and returns a readability \"score\" for the test based on the [Dale-Chall readability formula](https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula#Formula). Call your previous functions to calculate the number of sentences, total words, and number of difficult (not easy) words.\n",
    "- Don't forget to adjust the score if the text is more than 5% difficult words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calc_readability_score() function\n",
    "def calc_readability_score(str):\n",
    "    \"\"\"This function takes in a string of text and returns a readability 'score'\"\"\"\n",
    "    \n",
    "    number_of_sentences = count_sentences(str)\n",
    "    \n",
    "    list_of_words = extract_words(str)\n",
    "    number_of_words = len(list_of_words)\n",
    "    \n",
    "    number_of_easy_words = count_easy_words(list_of_words)\n",
    "    difficult_words = number_of_words - number_of_easy_words\n",
    "    \n",
    "    # Calculate the readability score\n",
    "    readability_score = (0.1579 * (difficult_words / number_of_words * 100)) + (0.0496 * (number_of_words / number_of_sentences))\n",
    "    \n",
    "    # Adjust the readability score if percentage of difficult words is above 5%\n",
    "    if (((number_of_words - number_of_easy_words) / number_of_words) * 100) > 5:\n",
    "        readability_score += 3.6365\n",
    "    \n",
    "    # Return statement\n",
    "    return readability_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the text of the `data/alice.txt` file (the full text of Alice in Wonderland) _as a single string_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from alice.txt\n",
    "with open('data/alice.txt', encoding=\"utf-8\") as alice:\n",
    "    alice_in_wonder = alice.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the readability score for the `alice.txt` file and print it out. Then print out the reading grade associated with that score. Use your previously-defined functions!\n",
    "- For testing, note that my calculations show `alice.txt` has 977 sentences and 27198 words, of which 3610 are difficult. This leads to a readability score of ~7.113."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Score of 'alice.txt': 7.113090902410715\n",
      "Grade 9th or 10th\n"
     ]
    }
   ],
   "source": [
    "# Calculate the readability score\n",
    "score_alice = calc_readability_score(alice_in_wonder)\n",
    "\n",
    "# Print the score\n",
    "print(\"Readability Score of 'alice.txt':\", score_alice)\n",
    "\n",
    "# Print the corresponding grade\n",
    "print_grade(score_alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that this result may not be an especially accurate model of a text's readability&mdash;after all, it's just based on a simple estimation!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
